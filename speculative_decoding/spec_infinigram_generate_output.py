import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel
import numpy as np
import pexpect
import re
import time
import io
import sys
import contextlib
import csv
import os

# Initialize byte-level model
byte_model_name = "JunxiongWang/MambaByte_PG19_972M"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
byte_model = MambaLMHeadModel.from_pretrained(byte_model_name).to(device)

# Define list of prefixes
prefixes = [
    "similar species . <unk>",
    "game on April 4 , 2011",
    "to the state of New York",
    "became Nero 's mistress in",
    "Citizens Defence Association ( <unk",
    "at the stern . It over",
    ". <unk> was later",
    "archaeological exploration of La Venta",
    "metres to the west of Monument",
    "in ' A ' Squadron were"    
]

# Define the function to run the model
def run_model(prefix, byte_model, device):
    f = io.StringIO()
    with contextlib.redirect_stdout(f):
        start_time = time.perf_counter()

        # Define input parameters
        draft_block_size = 3  # Number of tokens to draft per iteration
        verify_tolerance = 3  # Tolerance for top-k verification
        max_generated_bytes = 124  # Maximum bytes to generate (excluding prefix)

        generated_text = prefix
        byte_input_ids = torch.tensor(list(prefix.encode('utf-8')), dtype=torch.long).unsqueeze(0).to(device)

        # Initialize total generated byte count (excluding prefix)
        total_generated_bytes = 0

        # Function to interact with Infinigram to get the next token
        def get_infinigram_output(prompt, num_tokens):
            process = pexpect.spawn(
                "./infinigram --train_file extracted_text_2_5B_chars.txt --out_dir output --tokenizer_config tokenizer.json",
                encoding='utf-8'
            )

            process.expect("enter query:")
            process.sendline(prompt)
            output = ""
            while True:
                try:
                    index = process.expect(["enter query:", pexpect.EOF, pexpect.TIMEOUT], timeout=5)
                    output += process.before
                    if index == 0:
                        break
                    elif index == 1:
                        break
                    elif index == 2:
                        break
                except pexpect.EOF:
                    break
            process.close()
            # Parse output to get top candidates
            matches = re.findall(r"p=([\d.e-]+) \(\d+/\d+\), k=\d+: (.+)", output)
            candidates = []
            for match in matches:
                probability = float(match[0])
                candidate_text = match[1].strip()
                candidates.append((candidate_text, probability))
                if len(candidates) >= num_tokens:
                    break
            return candidates

        # Main loop
        while total_generated_bytes < max_generated_bytes:
            # Step 1: Use Infinigram to draft tokens
            draft_candidates = get_infinigram_output(generated_text, draft_block_size)
            if not draft_candidates:
                break  
            # Get draft text 
            draft_text = ''.join([candidate[0] for candidate in draft_candidates])
            draft_bytes = list(draft_text.encode('utf-8'))

            total_byte_input = torch.cat(
                [byte_input_ids, torch.tensor(draft_bytes, dtype=torch.long).unsqueeze(0).to(device)],
                dim=-1
            )

            # Step 2: Verify drafted bytes with the byte-level model
            byte_outputs = byte_model(input_ids=total_byte_input)
            # Get logits
            if hasattr(byte_outputs, 'logits'):
                logits_byte = byte_outputs.logits.squeeze(0)
            else:
                hidden_states = byte_outputs
                logits_byte = byte_model.lm_head(hidden_states).squeeze(0)

            # Step 3: Find branching position c
            c = -1
            for i in range(len(draft_bytes)):
                # Check if the maximum number of generated bytes has been reached
                if total_generated_bytes >= max_generated_bytes:
                    break  
                byte_logits = logits_byte[byte_input_ids.size(1) - 1 + i]
                top_k_indices = torch.topk(byte_logits, k=verify_tolerance).indices
                if draft_bytes[i] in top_k_indices.cpu().numpy():
                    total_generated_bytes += 1  
                    continue
                else:
                    c = i
                    break
            if c == -1 or total_generated_bytes >= max_generated_bytes:
                c = i
                
            # Step 4: Discard bytes after position c
            verified_bytes = draft_bytes[:c]

            # Step 5: Correct bytes starting from position c+1
            corrected_bytes = verified_bytes.copy()
            byte_input_for_correction = torch.cat(
                [byte_input_ids, torch.tensor(corrected_bytes, dtype=torch.long).unsqueeze(0).to(device)],
                dim=-1
            )
            boundary_token = ord(' ')  # Update generated byte count
            boundary_reached = False

            corrected_bytes_generated = []

            while not boundary_reached and total_generated_bytes < max_generated_bytes:
                # 生成下一个字节
                byte_outputs = byte_model(input_ids=byte_input_for_correction)
                # 获取logits
                if hasattr(byte_outputs, 'logits'):
                    byte_logits = byte_outputs.logits[:, -1, :].squeeze(0)
                else:
                    hidden_states = byte_outputs
                    byte_logits = byte_model.lm_head(hidden_states)[:, -1, :].squeeze(0)
                next_byte = torch.argmax(byte_logits).item()
                corrected_bytes_generated.append(next_byte)
                total_generated_bytes += 1  

                if next_byte == boundary_token:
                    boundary_reached = True
                # Update input
                byte_input_for_correction = torch.cat(
                    [byte_input_for_correction, torch.tensor([[next_byte]], dtype=torch.long).to(device)],
                    dim=-1
                )

            # Step 6: Merge verified and corrected bytes
            corrected_bytes.extend(corrected_bytes_generated)
            generated_text_segment = bytes(corrected_bytes_generated).decode('utf-8', errors='ignore')
            generated_text += generated_text_segment

            # Update byte_input_ids
            byte_input_ids = torch.tensor(
                list(generated_text.encode('utf-8')), dtype=torch.long
            ).unsqueeze(0).to(device)

            if total_generated_bytes >= max_generated_bytes:
                break  # Reached maximum bytes, exit loop

        end_time = time.perf_counter()
        total_time = end_time - start_time

    return total_time, generated_text

# Run model and save results
results = []

for prefix in prefixes:
    total_time, printed_output = run_model(prefix, byte_model, device)
    results.append((prefix, total_time, printed_output))

# Save results to CSV file
with open('spec_infinigram_results.csv', 'a', encoding='utf-8', newline='') as csvfile:
    csv_writer = csv.writer(csvfile)
    csv_writer.writerow(["Prefix", "Total Time (seconds)", "Final Generated Text"])
    for prefix, total_time, generated_text in results:
        csv_writer.writerow([prefix, f"{total_time:.2f}", generated_text])





