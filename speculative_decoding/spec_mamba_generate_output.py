import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel
import numpy as np
import time
import io
import contextlib
import csv

# Initialize models and tokenizer
subword_model_name = "state-spaces/mamba-130m-hf"
byte_model_name = "JunxiongWang/MambaByte_PG19_972M"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load models and tokenizer onto the specified device
subword_model = AutoModelForCausalLM.from_pretrained(subword_model_name).to(device)
subword_tokenizer = AutoTokenizer.from_pretrained(subword_model_name)
byte_model = MambaLMHeadModel.from_pretrained(byte_model_name).to(device)

# Define list of prefixes
prefixes = [
    "a part . It bears the",
    "11 , but lost in another",
    "the JMA upgraded the depression",
    "Herbert , and worked to",
    "1916 volumes , making him the",
    ". <unk> , <",
    "Turret No. 5 that",
    "Armour ( shoes ) , Muscle",
    "be needed came on the morning",
    "when dipped in water , will"    
]

# Define the function to run the model
def run_model(prefix, subword_model, subword_tokenizer, byte_model, device):
    # Redirect standard output to capture printed results
    f = io.StringIO()
    with contextlib.redirect_stdout(f):
        start_time = time.perf_counter()

        # Define input parameters
        draft_block_size = 3  # Number of tokens to draft per iteration
        verify_tolerance = 3  # Tolerance for top-k verification
        max_generated_bytes = 124  # Maximum generated bytes (excluding prefix)

        # Initialize inputs
        subword_input_ids = subword_tokenizer(prefix, return_tensors="pt").input_ids.to(device)
        byte_input_ids = torch.tensor(list(prefix.encode('utf-8')), dtype=torch.long).unsqueeze(0).to(device)

        # Initialize hidden state cache
        cached_subword_hidden_state = None

        # Initialize output buffer
        generated_text = prefix

        # Initialize total generated bytes count (excluding prefix)
        total_generated_bytes = 0

        # Main loop
        while total_generated_bytes < max_generated_bytes:
            # Step 1: Use the subword model to generate draft subwords
            draft_subwords = []
            subword_attention_mask = torch.ones_like(subword_input_ids, dtype=torch.long).to(device)
            subword_outputs = subword_model(
                input_ids=subword_input_ids,
                attention_mask=subword_attention_mask,
                output_hidden_states=True
            )

            if cached_subword_hidden_state is not None:
                subword_hidden_states = cached_subword_hidden_state
            else:
                subword_hidden_states = subword_outputs.hidden_states

            # Generate draft tokens
            for _ in range(draft_block_size):
                next_token_logits = subword_outputs.logits[:, -1, :]
                next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)
                draft_subwords.append(next_token.item())
                subword_input_ids = torch.cat([subword_input_ids, next_token], dim=-1)
                subword_attention_mask = torch.ones_like(subword_input_ids, dtype=torch.long).to(device)
                subword_outputs = subword_model(
                    input_ids=subword_input_ids,
                    attention_mask=subword_attention_mask,
                    output_hidden_states=True
                )
                subword_hidden_states = subword_outputs.hidden_states

            # Convert draft subwords to bytes
            draft_subword_text = subword_tokenizer.decode(draft_subwords, skip_special_tokens=True)
            draft_bytes = list(draft_subword_text.encode('utf-8'))

            # Prepare input for the byte-level model
            total_byte_input = torch.cat(
                [byte_input_ids, torch.tensor(draft_bytes, dtype=torch.long).unsqueeze(0).to(device)],
                dim=-1
            )

            # Step 2: Verify draft bytes using the byte-level model
            byte_outputs = byte_model(input_ids=total_byte_input)
            # Obtain logits
            if hasattr(byte_outputs, 'logits'):
                logits_byte = byte_outputs.logits.squeeze(0)
            else:
                hidden_states = byte_outputs
                logits_byte = byte_model.lm_head(hidden_states).squeeze(0)

            # Step 3: Find the branching position c
            c = -1
            for i in range(len(draft_bytes)):
                # Check if total_generated_bytes has reached the maximum limit
                if total_generated_bytes >= max_generated_bytes:
                    c = i
                    break
                byte_logits = logits_byte[byte_input_ids.size(1) - 1 + i]  # Get logits for the current byte
                top_k_indices = torch.topk(byte_logits, k=verify_tolerance).indices
                if draft_bytes[i] in top_k_indices.cpu().numpy():
                    total_generated_bytes += 1
                    continue
                else:
                    c = i
                    break
            if c == -1:
                c = len(draft_bytes)  # All bytes were verified

            # Step 4: Discard bytes after position c
            verified_bytes = draft_bytes[:c]

            # Step 5: Correct starting from position c+1 using the byte model
            corrected_bytes = verified_bytes.copy()
            byte_input_for_correction = torch.cat(
                [byte_input_ids, torch.tensor(corrected_bytes, dtype=torch.long).unsqueeze(0).to(device)],
                dim=-1
            )
            boundary_token = ord(' ')  # Assume space is the boundary byte
            boundary_reached = False

            # Collect corrected generated bytes
            corrected_bytes_generated = []

            while not boundary_reached and total_generated_bytes < max_generated_bytes:
                # Generate the next byte
                byte_outputs = byte_model(input_ids=byte_input_for_correction)
                # Get logits
                if hasattr(byte_outputs, 'logits'):
                    byte_logits = byte_outputs.logits[:, -1, :].squeeze(0)
                else:
                    hidden_states = byte_outputs
                    byte_logits = byte_model.lm_head(hidden_states)[:, -1, :].squeeze(0)
                next_byte = torch.argmax(byte_logits).item()
                corrected_bytes_generated.append(next_byte)
                total_generated_bytes += 1  # Increment total generated bytes
                if next_byte == boundary_token:
                    boundary_reached = True
                # Update input
                byte_input_for_correction = torch.cat(
                    [byte_input_for_correction, torch.tensor([[next_byte]], dtype=torch.long).to(device)],
                    dim=-1
                )

            # Step 6: Merge verified and corrected bytes
            corrected_bytes.extend(corrected_bytes_generated)
            generated_text_segment = bytes(corrected_bytes_generated).decode('utf-8', errors='ignore')
            generated_text += generated_text_segment

            # Update byte_input_ids and subword_input_ids
            byte_input_ids = torch.tensor(
                list(generated_text.encode('utf-8')), dtype=torch.long
            ).unsqueeze(0).to(device)
            subword_input_ids = subword_tokenizer(generated_text, return_tensors="pt").input_ids.to(device)

            # Check if maximum byte count has been reached
            if total_generated_bytes >= max_generated_bytes:
                # Reached limit, stop generation
                break

            # Step 7: Cache hidden states for the next iteration
            cached_subword_hidden_state = subword_hidden_states

            # Check if end-of-sequence token was generated
            if subword_input_ids[0, -1].item() == subword_tokenizer.eos_token_id:
                break

        # End of generation loop
        end_time = time.perf_counter()
        total_time = end_time - start_time

    # Get the generated text from the captured output
    generated_text = generated_text  

    # Return total time and generated text
    return total_time, generated_text

# Run the model and save results
results = []

for prefix in prefixes:
    total_time, generated_text = run_model(prefix, subword_model, subword_tokenizer, byte_model, device)
    results.append((prefix, total_time, generated_text))

# Save results to CSV file
with open('spec_mamba_results.csv', 'w', encoding='utf-8', newline='') as csvfile:
    csv_writer = csv.writer(csvfile)
    csv_writer.writerow(["Prefix", "Total Time (seconds)", "Generated Text"])
    for prefix, total_time, generated_text in results:
        csv_writer.writerow([prefix, f"{total_time:.2f}", generated_text])

print("All prefixes processed and results saved to spec_mamba_results.csv.")
