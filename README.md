# Comparing the Performance of Neural Network and N-Gram as Draft Models for Speculative Decoding in MambaByte

## Project Overview
This project builds upon MambaByte's speculative decoding method, aiming to enhance text generation efficiency. MambaByte utilizes a subword-level drafting and byte-level verification strategy, with a neural network model (Mamba) serving as the draft model. Notably, n-gram models have the advantage of low latency. Thus, this project will explore whether replacing the neural network draft model in speculative decoding with an n-gram model, trained on the same dataset and with the same data volume, can improve MambaByte's generation speed without compromising the quality of the generated text.

## Project Structure
- `prompt_sample/`: Contains code for extracting prompts for text generation tasks from various datasets, as well as txt files with prompts generated by these scripts.
  - `pg19_common_phrases.py`: Extracts common phrases from the PG19 dataset.
  - `pg19_filtered_prefix.py`: Extracts prefixes that meet specific criteria from the PG19 dataset.
  - `wikitext_prefix_sample.py`: Randomly extracts prefixes from the WikiText dataset as prompts.

- `ngram_training/`: Code related to training the n-gram model.
  - `pile_download.py`: Downloads data for training.
  - `preprocessing.py`: Preprocesses the downloaded data, tokenizing and converting it into token IDs for training.

- `ngram_generation/`: Contains code for generating text with three n-gram models and the resulting outputs.
  - `bigram_generate_output.py`
  - `trigram_generate_output.py`
  - `four_gram_generate_output.py`
  - `ave_time_calculate.py`: Calculates the average time taken for text generation by each model.

- `infinigram_generation/`: Contains code for text generation using the infini-gram model and its output results.
  - `infinigram_generate_output_10tokens.py`

- `mamba_generation/`: Contains code for text generation using the Mamba and MambaByte models and their output results.
  - `mambabyte_generate_output_124bytes_sample.py`
  - `mamba_generate_output_30tokens_gpu.py` 
  - `mamba_generate_output_30tokens.py`

- `speculative_decoding/`: Contains code for speculative decoding with mamba and infini-gram as draft models, along with output results.
  - `spec_infinigram_generate_output.py`
  - `spec_mamba_generate_output.py`
  
## N-Gram Model Training
The n-gram model training in this project is based on the [official KenLM tutorial](https://github.com/kpu/kenlm?tab=readme-ov-file). Following KenLMâ€™s instructions, I used its tools to build and train the n-gram model to support text generation tasks in this project.

## InfiniGram Model Training
The InfiniGram model training in this project is based on the [unofficial InfiniGram tutorial](https://github.com/AlexWan0/infini-gram/tree/main) in the `go_rust_tokenizer` branch. Following this tutorial, I used the provided steps and tools to configure and train the InfiniGram model to support text generation tasks in this project.




